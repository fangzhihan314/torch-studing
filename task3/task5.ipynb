{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# å·ç§¯ç¥ç»ç½‘ç»œåŸºç¡€\n",
    "\n",
    "### äºŒç»´äº’ç›¸å…³è¿ç®—\n",
    "\n",
    "äºŒç»´äº’ç›¸å…³ï¼ˆcross-correlationï¼‰è¿ç®—çš„è¾“å…¥æ˜¯ä¸€ä¸ªäºŒç»´è¾“å…¥æ•°ç»„å’Œä¸€ä¸ªäºŒç»´æ ¸ï¼ˆkernelï¼‰æ•°ç»„ï¼Œè¾“å‡ºä¹Ÿæ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œå…¶ä¸­æ ¸æ•°ç»„é€šå¸¸ç§°ä¸ºå·ç§¯æ ¸æˆ–è¿‡æ»¤å™¨ï¼ˆfilterï¼‰ã€‚å·ç§¯æ ¸çš„å°ºå¯¸é€šå¸¸å°äºè¾“å…¥æ•°ç»„ï¼Œå·ç§¯æ ¸åœ¨è¾“å…¥æ•°ç»„ä¸Šæ»‘åŠ¨ï¼Œåœ¨æ¯ä¸ªä½ç½®ä¸Šï¼Œå·ç§¯æ ¸ä¸è¯¥ä½ç½®å¤„çš„è¾“å…¥å­æ•°ç»„æŒ‰å…ƒç´ ç›¸ä¹˜å¹¶æ±‚å’Œï¼Œå¾—åˆ°è¾“å‡ºæ•°ç»„ä¸­ç›¸åº”ä½ç½®çš„å…ƒç´ ã€‚å›¾1å±•ç¤ºäº†ä¸€ä¸ªäº’ç›¸å…³è¿ç®—çš„ä¾‹å­ï¼Œé˜´å½±éƒ¨åˆ†åˆ†åˆ«æ˜¯è¾“å…¥çš„ç¬¬ä¸€ä¸ªè®¡ç®—åŒºåŸŸã€æ ¸æ•°ç»„ä»¥åŠå¯¹åº”çš„è¾“å‡ºã€‚\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5nfdbhcw5.png?imageView2/0/w/640/h/640)\n",
    "å›¾1 äºŒç»´äº’ç›¸å…³è¿ç®—\n",
    "\n",
    "ä¸‹é¢æˆ‘ä»¬ç”¨`corr2d`å‡½æ•°å®ç°äºŒç»´äº’ç›¸å…³è¿ç®—ï¼Œå®ƒæ¥å—è¾“å…¥æ•°ç»„`X`ä¸æ ¸æ•°ç»„`K`ï¼Œå¹¶è¾“å‡ºæ•°ç»„`Y`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#äºŒç»´äº’ç›¸å…³å®ç°\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "def corr2d(X, K):\n",
    "    H, W = X.shape\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros(H - h + 1, W - w + 1)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#äºŒç»´å·ç§¯å®ç°\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(Conv2D, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å·ç§¯å±‚çš„ç®€æ´å®ç°\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨Pytorchä¸­çš„`nn.Conv2d`ç±»æ¥å®ç°äºŒç»´å·ç§¯å±‚ï¼Œä¸»è¦å…³æ³¨ä»¥ä¸‹å‡ ä¸ªæ„é€ å‡½æ•°å‚æ•°ï¼š\n",
    "\n",
    "* `in_channels` (python:int) â€“ Number of channels in the input imag\n",
    "* `out_channels` (python:int) â€“ Number of channels produced by the convolution\n",
    "* `kernel_size` (python:int or tuple) â€“ Size of the convolving kernel\n",
    "* `stride` (python:int or tuple, optional) â€“ Stride of the convolution. Default: 1\n",
    "* `padding` (python:int or tuple, optional) â€“ Zero-padding added to both sides of the input. Default: 0\n",
    "* `bias` (bool, optional) â€“ If True, adds a learnable bias to the output. Default: True\n",
    "\n",
    "`forward`å‡½æ•°çš„å‚æ•°ä¸ºä¸€ä¸ªå››ç»´å¼ é‡ï¼Œå½¢çŠ¶ä¸º$(N, C_{in}, H_{in}, W_{in})$ï¼Œè¿”å›å€¼ä¹Ÿæ˜¯ä¸€ä¸ªå››ç»´å¼ é‡ï¼Œå½¢çŠ¶ä¸º$(N, C_{out}, H_{out}, W_{out})$ï¼Œå…¶ä¸­$N$æ˜¯æ‰¹é‡å¤§å°ï¼Œ$C, H, W$åˆ†åˆ«è¡¨ç¤ºé€šé“æ•°ã€é«˜åº¦ã€å®½åº¦ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å„ç§ç»å…¸ç½‘ç»œæ„å»ºå®ç°è®°å½•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Alexnet\n",
    "class Flatten(torch.nn.Module):  #å±•å¹³æ“ä½œ\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "class Reshape(torch.nn.Module): #å°†å›¾åƒå¤§å°é‡å®šå‹\n",
    "    def forward(self, x):\n",
    "        return x.view(-1,1,28,28)      #(B x C x H x W)\n",
    "    \n",
    "net = torch.nn.Sequential(     #Lelet                                                  \n",
    "    Reshape(),\n",
    "    nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2), #b*1*28*28  =>b*6*28*28\n",
    "    nn.Sigmoid(),                                                       \n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),                              #b*6*28*28  =>b*6*14*14\n",
    "    nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),           #b*6*14*14  =>b*16*10*10\n",
    "    nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),                              #b*16*10*10  => b*16*5*5\n",
    "    Flatten(),                                                          #b*16*5*5   => b*400\n",
    "    nn.Linear(in_features=16*5*5, out_features=120),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(84, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#VGG\n",
    "def vgg_block(num_convs, in_channels, out_channels): #å·ç§¯å±‚ä¸ªæ•°ï¼Œè¾“å…¥é€šé“æ•°ï¼Œè¾“å‡ºé€šé“æ•°\n",
    "    blk = []\n",
    "    for i in range(num_convs):\n",
    "        if i == 0:\n",
    "            blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        else:\n",
    "            blk.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        blk.append(nn.ReLU())\n",
    "    blk.append(nn.MaxPool2d(kernel_size=2, stride=2)) # è¿™é‡Œä¼šä½¿å®½é«˜å‡åŠ\n",
    "    return nn.Sequential(*blk)\n",
    "conv_arch = ((1, 1, 64), (1, 64, 128), (2, 128, 256), (2, 256, 512), (2, 512, 512))\n",
    "# ç»è¿‡5ä¸ªvgg_block, å®½é«˜ä¼šå‡åŠ5æ¬¡, å˜æˆ 224/32 = 7\n",
    "fc_features = 512 * 7 * 7 # c * w * h\n",
    "fc_hidden_units = 4096 # ä»»æ„\n",
    "def vgg(conv_arch, fc_features, fc_hidden_units=4096):\n",
    "    net = nn.Sequential()\n",
    "    # å·ç§¯å±‚éƒ¨åˆ†\n",
    "    for i, (num_convs, in_channels, out_channels) in enumerate(conv_arch):\n",
    "        # æ¯ç»è¿‡ä¸€ä¸ªvgg_blockéƒ½ä¼šä½¿å®½é«˜å‡åŠ\n",
    "        net.add_module(\"vgg_block_\" + str(i+1), vgg_block(num_convs, in_channels, out_channels))\n",
    "    # å…¨è¿æ¥å±‚éƒ¨åˆ†\n",
    "    net.add_module(\"fc\", nn.Sequential(d2l.FlattenLayer(),\n",
    "                                 nn.Linear(fc_features, fc_hidden_units),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.5),\n",
    "                                 nn.Linear(fc_hidden_units, fc_hidden_units),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.5),\n",
    "                                 nn.Linear(fc_hidden_units, 10)\n",
    "                                ))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GoogleNet\n",
    "class Inception(nn.Module):\n",
    "    # c1 - c4ä¸ºæ¯æ¡çº¿è·¯é‡Œçš„å±‚çš„è¾“å‡ºé€šé“æ•°\n",
    "    def __init__(self, in_c, c1, c2, c3, c4):\n",
    "        super(Inception, self).__init__()\n",
    "        # çº¿è·¯1ï¼Œå•1 x 1å·ç§¯å±‚\n",
    "        self.p1_1 = nn.Conv2d(in_c, c1, kernel_size=1)\n",
    "        # çº¿è·¯2ï¼Œ1 x 1å·ç§¯å±‚åæ¥3 x 3å·ç§¯å±‚\n",
    "        self.p2_1 = nn.Conv2d(in_c, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # çº¿è·¯3ï¼Œ1 x 1å·ç§¯å±‚åæ¥5 x 5å·ç§¯å±‚\n",
    "        self.p3_1 = nn.Conv2d(in_c, c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # çº¿è·¯4ï¼Œ3 x 3æœ€å¤§æ± åŒ–å±‚åæ¥1 x 1å·ç§¯å±‚\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_c, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        return torch.cat((p1, p2, p3, p4), dim=1)  # åœ¨é€šé“ç»´ä¸Šè¿ç»“è¾“å‡º\n",
    "    \n",
    "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),\n",
    "                   Inception(256, 128, (128, 192), (32, 96), 64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),\n",
    "                   Inception(512, 160, (112, 224), (24, 64), 64),\n",
    "                   Inception(512, 128, (128, 256), (24, 64), 64),\n",
    "                   Inception(512, 112, (144, 288), (32, 64), 64),\n",
    "                   Inception(528, 256, (160, 320), (32, 128), 128),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception(832, 384, (192, 384), (48, 128), 128),\n",
    "                   d2l.GlobalAvgPool2d())\n",
    "\n",
    "net = nn.Sequential(b1, b2, b3, b4, b5, \n",
    "                    d2l.FlattenLayer(), nn.Linear(1024, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pythonä¸­superçš„ä½œç”¨ï¼š\n",
    "ä¸éœ€è¦æ˜ç¡®ç»™å‡ºä»»ä½•åŸºç±»çš„åå­—,å®ƒä¼šè‡ªåŠ¨æ‰¾åˆ°æ‰€æœ‰ç›´æ¥åŸºç±»,åŠå…¶å¯¹åº”çš„æ–¹æ³•.ç”¨äºç»§æ‰¿.\n",
    "\n",
    "super æ˜¯ç”¨æ¥è§£å†³å¤šé‡ç»§æ‰¿é—®é¢˜çš„ï¼Œç›´æ¥ç”¨ç±»åè°ƒç”¨çˆ¶ç±»æ–¹æ³•åœ¨ä½¿ç”¨å•ç»§æ‰¿çš„æ—¶å€™æ²¡é—®é¢˜ï¼Œä½†æ˜¯å¦‚æœä½¿ç”¨å¤šç»§æ‰¿ï¼Œä¼šæ¶‰åŠåˆ°æŸ¥æ‰¾é¡ºåºï¼ˆMROï¼‰ã€é‡å¤è°ƒç”¨ï¼ˆé’»çŸ³ç»§æ‰¿ï¼‰ç­‰ç§ç§é—®é¢˜ã€‚\n",
    "\n",
    "\n",
    "torch.nnæ˜¯ä¸“é—¨ä¸ºç¥ç»ç½‘ç»œè®¾è®¡çš„æ¨¡å—åŒ–æ¥å£ã€‚nnæ„å»ºäºautogradä¹‹ä¸Šï¼Œå¯ä»¥ç”¨æ¥å®šä¹‰å’Œè¿è¡Œç¥ç»ç½‘ç»œã€‚\n",
    "nn.Moduleæ˜¯nnä¸­ååˆ†é‡è¦çš„ç±»,åŒ…å«ç½‘ç»œå„å±‚çš„å®šä¹‰åŠforwardæ–¹æ³•ã€‚\n",
    "å®šä¹‰è‡ªå·²çš„ç½‘ç»œï¼š\n",
    "    éœ€è¦ç»§æ‰¿nn.Moduleç±»ï¼Œå¹¶å®ç°forwardæ–¹æ³•ã€‚\n",
    "    ä¸€èˆ¬æŠŠç½‘ç»œä¸­å…·æœ‰å¯å­¦ä¹ å‚æ•°çš„å±‚æ”¾åœ¨æ„é€ å‡½æ•°__init__()ä¸­ï¼Œ\n",
    "    ä¸å…·æœ‰å¯å­¦ä¹ å‚æ•°çš„å±‚(å¦‚ReLU)å¯æ”¾åœ¨æ„é€ å‡½æ•°ä¸­ï¼Œä¹Ÿå¯ä¸æ”¾åœ¨æ„é€ å‡½æ•°ä¸­(è€Œåœ¨forwardä¸­ä½¿ç”¨nn.functionalæ¥ä»£æ›¿)\n",
    "    \n",
    "    åªè¦åœ¨nn.Moduleçš„å­ç±»ä¸­å®šä¹‰äº†forwardå‡½æ•°ï¼Œbackwardå‡½æ•°å°±ä¼šè¢«è‡ªåŠ¨å®ç°(åˆ©ç”¨Autograd)ã€‚\n",
    "    åœ¨forwardå‡½æ•°ä¸­å¯ä»¥ä½¿ç”¨ä»»ä½•Variableæ”¯æŒçš„å‡½æ•°ï¼Œæ¯•ç«Ÿåœ¨æ•´ä¸ªpytorchæ„å»ºçš„å›¾ä¸­ï¼Œæ˜¯Variableåœ¨æµåŠ¨ã€‚è¿˜å¯ä»¥ä½¿ç”¨\n",
    "    if,for,print,logç­‰pythonè¯­æ³•.\n",
    "    \n",
    "    æ³¨ï¼šPytorchåŸºäºnn.Moduleæ„å»ºçš„æ¨¡å‹ä¸­ï¼Œåªæ”¯æŒmini-batchçš„Variableè¾“å…¥æ–¹å¼ï¼Œ\n",
    "    æ¯”å¦‚ï¼Œåªæœ‰ä¸€å¼ è¾“å…¥å›¾ç‰‡ï¼Œä¹Ÿéœ€è¦å˜æˆ N x C x H x W çš„å½¢å¼ï¼š\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ³¨æ„åŠ›æœºåˆ¶\n",
    "åœ¨â€œç¼–ç å™¨â€”è§£ç å™¨ï¼ˆseq2seqï¼‰â€â¼€èŠ‚â¾¥ï¼Œè§£ç å™¨åœ¨å„ä¸ªæ—¶é—´æ­¥ä¾èµ–ç›¸åŒçš„èƒŒæ™¯å˜é‡ï¼ˆcontext vectorï¼‰æ¥è·å–è¾“â¼Šåºåˆ—ä¿¡æ¯ã€‚å½“ç¼–ç å™¨ä¸ºå¾ªç¯ç¥ç»â½¹ç»œæ—¶ï¼ŒèƒŒæ™¯å˜é‡æ¥â¾ƒå®ƒæœ€ç»ˆæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ã€‚å°†æºåºåˆ—è¾“å…¥ä¿¡æ¯ä»¥å¾ªç¯å•ä½çŠ¶æ€ç¼–ç ï¼Œç„¶åå°†å…¶ä¼ é€’ç»™è§£ç å™¨ä»¥ç”Ÿæˆç›®æ ‡åºåˆ—ã€‚ç„¶è€Œè¿™ç§ç»“æ„å­˜åœ¨ç€é—®é¢˜ï¼Œå°¤å…¶æ˜¯RNNæœºåˆ¶å®é™…ä¸­å­˜åœ¨é•¿ç¨‹æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ï¼Œå¯¹äºè¾ƒé•¿çš„å¥å­ï¼Œæˆ‘ä»¬å¾ˆéš¾å¯„å¸Œæœ›äºå°†è¾“å…¥çš„åºåˆ—è½¬åŒ–ä¸ºå®šé•¿çš„å‘é‡è€Œä¿å­˜æ‰€æœ‰çš„æœ‰æ•ˆä¿¡æ¯ï¼Œæ‰€ä»¥éšç€æ‰€éœ€ç¿»è¯‘å¥å­çš„é•¿åº¦çš„å¢åŠ ï¼Œè¿™ç§ç»“æ„çš„æ•ˆæœä¼šæ˜¾è‘—ä¸‹é™ã€‚\n",
    "\n",
    "ä¸æ­¤åŒæ—¶ï¼Œè§£ç çš„ç›®æ ‡è¯è¯­å¯èƒ½åªä¸åŸè¾“å…¥çš„éƒ¨åˆ†è¯è¯­æœ‰å…³ï¼Œè€Œå¹¶ä¸æ˜¯ä¸æ‰€æœ‰çš„è¾“å…¥æœ‰å…³ã€‚ä¾‹å¦‚ï¼Œå½“æŠŠâ€œHello worldâ€ç¿»è¯‘æˆâ€œBonjour le mondeâ€æ—¶ï¼Œâ€œHelloâ€æ˜ å°„æˆâ€œBonjourâ€ï¼Œâ€œworldâ€æ˜ å°„æˆâ€œmondeâ€ã€‚åœ¨seq2seqæ¨¡å‹ä¸­ï¼Œè§£ç å™¨åªèƒ½éšå¼åœ°ä»ç¼–ç å™¨çš„æœ€ç»ˆçŠ¶æ€ä¸­é€‰æ‹©ç›¸åº”çš„ä¿¡æ¯ã€‚ç„¶è€Œï¼Œæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥å°†è¿™ç§é€‰æ‹©è¿‡ç¨‹æ˜¾å¼åœ°å»ºæ¨¡ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ³¨æ„åŠ›æœºåˆ¶æ¡†æ¶\n",
    "\n",
    "Attention æ˜¯ä¸€ç§é€šç”¨çš„å¸¦æƒæ± åŒ–æ–¹æ³•ï¼Œè¾“å…¥ç”±ä¸¤éƒ¨åˆ†æ„æˆï¼šè¯¢é—®ï¼ˆqueryï¼‰å’Œé”®å€¼å¯¹ï¼ˆkey-value pairsï¼‰ã€‚$ğ¤_ğ‘–âˆˆâ„^{ğ‘‘_ğ‘˜}, ğ¯_ğ‘–âˆˆâ„^{ğ‘‘_ğ‘£}$. Query  $ğªâˆˆâ„^{ğ‘‘_ğ‘}$ , attention layerå¾—åˆ°è¾“å‡ºä¸valueçš„ç»´åº¦ä¸€è‡´ $ğ¨âˆˆâ„^{ğ‘‘_ğ‘£}$. å¯¹äºä¸€ä¸ªqueryæ¥è¯´ï¼Œattention layer ä¼šä¸æ¯ä¸€ä¸ªkeyè®¡ç®—æ³¨æ„åŠ›åˆ†æ•°å¹¶è¿›è¡Œæƒé‡çš„å½’ä¸€åŒ–ï¼Œè¾“å‡ºçš„å‘é‡$o$åˆ™æ˜¯valueçš„åŠ æƒæ±‚å’Œï¼Œè€Œæ¯ä¸ªkeyè®¡ç®—çš„æƒé‡ä¸valueä¸€ä¸€å¯¹åº”ã€‚\n",
    "\n",
    "ä¸ºäº†è®¡ç®—è¾“å‡ºï¼Œæˆ‘ä»¬é¦–å…ˆå‡è®¾æœ‰ä¸€ä¸ªå‡½æ•°$\\alpha$ ç”¨äºè®¡ç®—queryå’Œkeyçš„ç›¸ä¼¼æ€§ï¼Œç„¶åå¯ä»¥è®¡ç®—æ‰€æœ‰çš„ attention scores $a_1, \\ldots, a_n$ by\n",
    "\n",
    "\n",
    "$$\n",
    "a_i = \\alpha(\\mathbf q, \\mathbf k_i).\n",
    "$$\n",
    "\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨ softmaxå‡½æ•° è·å¾—æ³¨æ„åŠ›æƒé‡ï¼š\n",
    "\n",
    "\n",
    "$$\n",
    "b_1, \\ldots, b_n = \\textrm{softmax}(a_1, \\ldots, a_n).\n",
    "$$\n",
    "\n",
    "\n",
    "æœ€ç»ˆçš„è¾“å‡ºå°±æ˜¯valueçš„åŠ æƒæ±‚å’Œï¼š\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf o = \\sum_{i=1}^n b_i \\mathbf v_i.\n",
    "$$\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5km4ooyu2.PNG?imageView2/0/w/960/h/960)\n",
    "\n",
    "ä¸åŒçš„attetion layerçš„åŒºåˆ«åœ¨äºscoreå‡½æ•°çš„é€‰æ‹©ï¼Œåœ¨æœ¬èŠ‚çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†è®¨è®ºä¸¤ä¸ªå¸¸ç”¨çš„æ³¨æ„å±‚ Dot-product Attention å’Œ Multilayer Perceptron Attentionï¼›éšåæˆ‘ä»¬å°†å®ç°ä¸€ä¸ªå¼•å…¥attentionçš„seq2seqæ¨¡å‹å¹¶åœ¨è‹±æ³•ç¿»è¯‘è¯­æ–™ä¸Šè¿›è¡Œè®­ç»ƒä¸æµ‹è¯•ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç‚¹ç§¯æ³¨æ„åŠ›\n",
    "The dot product å‡è®¾queryå’Œkeysæœ‰ç›¸åŒçš„ç»´åº¦, å³ $\\forall i, ğª,ğ¤_ğ‘– âˆˆ â„_ğ‘‘ $. é€šè¿‡è®¡ç®—queryå’Œkeyè½¬ç½®çš„ä¹˜ç§¯æ¥è®¡ç®—attention score,é€šå¸¸è¿˜ä¼šé™¤å» $\\sqrt{d}$ å‡å°‘è®¡ç®—å‡ºæ¥çš„scoreå¯¹ç»´åº¦ğ‘‘çš„ä¾èµ–æ€§ï¼Œå¦‚ä¸‹\n",
    "\n",
    "\n",
    "$$\n",
    "ğ›¼(ğª,ğ¤)=âŸ¨ğª,ğ¤âŸ©/ \\sqrt{d} \n",
    "$$\n",
    "\n",
    "å‡è®¾ $ ğâˆˆâ„^{ğ‘šÃ—ğ‘‘}$ æœ‰ $m$ ä¸ªqueryï¼Œ$ğŠâˆˆâ„^{ğ‘›Ã—ğ‘‘}$ æœ‰ $n$ ä¸ªkeys. æˆ‘ä»¬å¯ä»¥é€šè¿‡çŸ©é˜µè¿ç®—çš„æ–¹å¼è®¡ç®—æ‰€æœ‰ $mn$ ä¸ªscoreï¼š\n",
    "\n",
    "\n",
    "$$\n",
    "ğ›¼(ğ,ğŠ)=ğğŠ^ğ‘‡/\\sqrt{d}\n",
    "$$\n",
    " \n",
    "ç°åœ¨è®©æˆ‘ä»¬å®ç°è¿™ä¸ªå±‚ï¼Œå®ƒæ”¯æŒä¸€æ‰¹æŸ¥è¯¢å’Œé”®å€¼å¯¹ã€‚æ­¤å¤–ï¼Œå®ƒæ”¯æŒä½œä¸ºæ­£åˆ™åŒ–éšæœºåˆ é™¤ä¸€äº›æ³¨æ„åŠ›æƒé‡."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save to the d2l package.\n",
    "class DotProductAttention(nn.Module): \n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # query: (batch_size, #queries, d)\n",
    "    # key: (batch_size, #kv_pairs, d)\n",
    "    # value: (batch_size, #kv_pairs, dim_v)\n",
    "    # valid_length: either (batch_size, ) or (batch_size, xx)\n",
    "    def forward(self, query, key, value, valid_length=None):\n",
    "        d = query.shape[-1]\n",
    "        # set transpose_b=True to swap the last two dimensions of key\n",
    "        \n",
    "        scores = torch.bmm(query, key.transpose(1,2)) / math.sqrt(d)\n",
    "        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n",
    "        print(\"attention_weight\\n\",attention_weights)\n",
    "        return torch.bmm(attention_weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å¤šå±‚æ„ŸçŸ¥æœºæ³¨æ„åŠ›\n",
    "åœ¨å¤šå±‚æ„ŸçŸ¥å™¨ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå°† query and keys æŠ•å½±åˆ°  $â„^â„$ .ä¸ºäº†æ›´å…·ä½“ï¼Œæˆ‘ä»¬å°†å¯ä»¥å­¦ä¹ çš„å‚æ•°åšå¦‚ä¸‹æ˜ å°„ \n",
    "$ğ–_ğ‘˜âˆˆâ„^{â„Ã—ğ‘‘_ğ‘˜}$ ,  $ğ–_ğ‘âˆˆâ„^{â„Ã—ğ‘‘_ğ‘}$ , and  $ğ¯âˆˆâ„^h$ . å°†scoreå‡½æ•°å®šä¹‰\n",
    "$$\n",
    "ğ›¼(ğ¤,ğª)=ğ¯^ğ‘‡tanh(ğ–_ğ‘˜ğ¤+ğ–_ğ‘ğª)\n",
    "$$\n",
    ". \n",
    "ç„¶åå°†key å’Œ value åœ¨ç‰¹å¾çš„ç»´åº¦ä¸Šåˆå¹¶ï¼ˆconcatenateï¼‰ï¼Œç„¶åé€è‡³ a single hidden layer perceptron è¿™å±‚ä¸­ hidden layer ä¸º  â„  and è¾“å‡ºçš„sizeä¸º 1 .éšå±‚æ¿€æ´»å‡½æ•°ä¸ºtanhï¼Œæ— åç½®."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save to the d2l package.\n",
    "class MLPAttention(nn.Module):  \n",
    "    def __init__(self, units,ipt_dim,dropout, **kwargs):\n",
    "        super(MLPAttention, self).__init__(**kwargs)\n",
    "        # Use flatten=True to keep query's and key's 3-D shapes.\n",
    "        self.W_k = nn.Linear(ipt_dim, units, bias=False)\n",
    "        self.W_q = nn.Linear(ipt_dim, units, bias=False)\n",
    "        self.v = nn.Linear(units, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, valid_length):\n",
    "        query, key = self.W_k(query), self.W_q(key)\n",
    "        #print(\"size\",query.size(),key.size())\n",
    "        # expand query to (batch_size, #querys, 1, units), and key to\n",
    "        # (batch_size, 1, #kv_pairs, units). Then plus them with broadcast.\n",
    "        features = query.unsqueeze(2) + key.unsqueeze(1)\n",
    "        #print(\"features:\",features.size())  #--------------å¼€å¯\n",
    "        scores = self.v(features).squeeze(-1) \n",
    "        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n",
    "        return torch.bmm(attention_weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶çš„Seq2seqæ¨¡å‹\n",
    "\n",
    "æœ¬èŠ‚ä¸­å°†æ³¨æ„æœºåˆ¶æ·»åŠ åˆ°sequence to sequence æ¨¡å‹ä¸­ï¼Œä»¥æ˜¾å¼åœ°ä½¿ç”¨æƒé‡èšåˆstatesã€‚ä¸‹å›¾å±•ç¤ºencoding å’Œdecodingçš„æ¨¡å‹ç»“æ„ï¼Œåœ¨æ—¶é—´æ­¥ä¸ºtçš„æ—¶å€™ã€‚æ­¤åˆ»attention layerä¿å­˜ç€encoderingçœ‹åˆ°çš„æ‰€æœ‰ä¿¡æ¯â€”â€”å³encodingçš„æ¯ä¸€æ­¥è¾“å‡ºã€‚åœ¨decodingé˜¶æ®µï¼Œè§£ç å™¨çš„$t$æ—¶åˆ»çš„éšè—çŠ¶æ€è¢«å½“ä½œqueryï¼Œencoderçš„æ¯ä¸ªæ—¶é—´æ­¥çš„hidden statesä½œä¸ºkeyå’Œvalueè¿›è¡Œattentionèšåˆ. Attetion modelçš„è¾“å‡ºå½“ä½œæˆä¸Šä¸‹æ–‡ä¿¡æ¯context vectorï¼Œå¹¶ä¸è§£ç å™¨è¾“å…¥$D_t$æ‹¼æ¥èµ·æ¥ä¸€èµ·é€åˆ°è§£ç å™¨ï¼š\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5km7o8z93.PNG?imageView2/0/w/800/h/800)\n",
    "\n",
    "$$\n",
    "Fig1å…·æœ‰æ³¨æ„æœºåˆ¶çš„seq-to-seqæ¨¡å‹è§£ç çš„ç¬¬äºŒæ­¥\n",
    "$$\n",
    "\n",
    "\n",
    "ä¸‹å›¾å±•ç¤ºäº†seq2seqæœºåˆ¶çš„æ‰€ä»¥å±‚çš„å…³ç³»ï¼Œä¸‹é¢å±•ç¤ºäº†encoderå’Œdecoderçš„layerç»“æ„\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5km8dihlr.PNG?imageView2/0/w/800/h/800)\n",
    "\n",
    "$$\n",
    "Fig2å…·æœ‰æ³¨æ„æœºåˆ¶çš„seq-to-seqæ¨¡å‹ä¸­å±‚ç»“æ„\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "åœ¨ä¹‹å‰çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å·²ç»ä»‹ç»äº†ä¸»æµçš„ç¥ç»ç½‘ç»œæ¶æ„å¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNsï¼‰ã€‚è®©æˆ‘ä»¬è¿›è¡Œä¸€äº›å›é¡¾ï¼š\n",
    "\n",
    "- CNNs æ˜“äºå¹¶è¡ŒåŒ–ï¼Œå´ä¸é€‚åˆæ•æ‰å˜é•¿åºåˆ—å†…çš„ä¾èµ–å…³ç³»ã€‚\n",
    "- RNNs é€‚åˆæ•æ‰é•¿è·ç¦»å˜é•¿åºåˆ—çš„ä¾èµ–ï¼Œä½†æ˜¯å´éš¾ä»¥å®ç°å¹¶è¡ŒåŒ–å¤„ç†åºåˆ—ã€‚\n",
    "\n",
    "ä¸ºäº†æ•´åˆCNNå’ŒRNNçš„ä¼˜åŠ¿ï¼Œ[\\[Vaswani et al., 2017\\]](https://d2l.ai/chapter_references/zreferences.html#vaswani-shazeer-parmar-ea-2017) åˆ›æ–°æ€§åœ°ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶è®¾è®¡äº†Transformeræ¨¡å‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨attentionæœºåˆ¶å®ç°äº†å¹¶è¡ŒåŒ–æ•æ‰åºåˆ—ä¾èµ–ï¼Œå¹¶ä¸”åŒæ—¶å¤„ç†åºåˆ—çš„æ¯ä¸ªä½ç½®çš„tokensï¼Œä¸Šè¿°ä¼˜åŠ¿ä½¿å¾—Transformeræ¨¡å‹åœ¨æ€§èƒ½ä¼˜å¼‚çš„åŒæ—¶å¤§å¤§å‡å°‘äº†è®­ç»ƒæ—¶é—´ã€‚\n",
    "\n",
    "å›¾10.3.1å±•ç¤ºäº†Transformeræ¨¡å‹çš„æ¶æ„ï¼Œä¸9.7èŠ‚çš„seq2seqæ¨¡å‹ç›¸ä¼¼ï¼ŒTransformeråŒæ ·åŸºäºç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå…¶åŒºåˆ«ä¸»è¦åœ¨äºä»¥ä¸‹ä¸‰ç‚¹ï¼š\n",
    "1. Transformer blocksï¼šå°†seq2seqæ¨¡å‹é‡çš„å¾ªç¯ç½‘ç»œæ›¿æ¢ä¸ºäº†Transformer Blocksï¼Œè¯¥æ¨¡å—åŒ…å«ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚ï¼ˆMulti-head Attention Layersï¼‰ä»¥åŠä¸¤ä¸ªposition-wise feed-forward networksï¼ˆFFNï¼‰ã€‚å¯¹äºè§£ç å™¨æ¥è¯´ï¼Œå¦ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚è¢«ç”¨äºæ¥å—ç¼–ç å™¨çš„éšè—çŠ¶æ€ã€‚\n",
    "2. Add and normï¼šå¤šå¤´æ³¨æ„åŠ›å±‚å’Œå‰é¦ˆç½‘ç»œçš„è¾“å‡ºè¢«é€åˆ°ä¸¤ä¸ªâ€œadd and normâ€å±‚è¿›è¡Œå¤„ç†ï¼Œè¯¥å±‚åŒ…å«æ®‹å·®ç»“æ„ä»¥åŠå±‚å½’ä¸€åŒ–ã€‚\n",
    "3. Position encodingï¼šç”±äºè‡ªæ³¨æ„åŠ›å±‚å¹¶æ²¡æœ‰åŒºåˆ†å…ƒç´ çš„é¡ºåºï¼Œæ‰€ä»¥ä¸€ä¸ªä½ç½®ç¼–ç å±‚è¢«ç”¨äºå‘åºåˆ—å…ƒç´ é‡Œæ·»åŠ ä½ç½®ä¿¡æ¯ã€‚\n",
    "\n",
    "![Fig. 10.3.1 The Transformer architecture.](https://cdn.kesci.com/upload/image/q5kpbj2cj5.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "$$\n",
    "Fig.10.3.1\\ Transformer æ¶æ„.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¤šå¤´æ³¨æ„åŠ›å±‚\n",
    "\n",
    "åœ¨æˆ‘ä»¬è®¨è®ºå¤šå¤´æ³¨æ„åŠ›å±‚ä¹‹å‰ï¼Œå…ˆæ¥è¿…é€Ÿç†è§£ä»¥ä¸‹è‡ªæ³¨æ„åŠ›ï¼ˆself-attentionï¼‰çš„ç»“æ„ã€‚è‡ªæ³¨æ„åŠ›æ¨¡å‹æ˜¯ä¸€ä¸ªæ­£è§„çš„æ³¨æ„åŠ›æ¨¡å‹ï¼Œåºåˆ—çš„æ¯ä¸€ä¸ªå…ƒç´ å¯¹åº”çš„keyï¼Œvalueï¼Œqueryæ˜¯å®Œå…¨ä¸€è‡´çš„ã€‚å¦‚å›¾10.3.2 è‡ªæ³¨æ„åŠ›è¾“å‡ºäº†ä¸€ä¸ªä¸è¾“å…¥é•¿åº¦ç›¸åŒçš„è¡¨å¾åºåˆ—ï¼Œä¸å¾ªç¯ç¥ç»ç½‘ç»œç›¸æ¯”ï¼Œè‡ªæ³¨æ„åŠ›å¯¹æ¯ä¸ªå…ƒç´ è¾“å‡ºçš„è®¡ç®—æ˜¯å¹¶è¡Œçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥é«˜æ•ˆçš„å®ç°è¿™ä¸ªæ¨¡å—ã€‚\n",
    "\n",
    "![Fig. 10.3.2 è‡ªæ³¨æ„åŠ›ç»“æ„](https://cdn.kesci.com/upload/image/q5kpckv38q.png?imageView2/0/w/320/h/320)\n",
    "\n",
    "$$\n",
    "Fig.10.3.2\\ è‡ªæ³¨æ„åŠ›ç»“æ„\n",
    "$$\n",
    "\n",
    "\n",
    "å¤šå¤´æ³¨æ„åŠ›å±‚åŒ…å«$h$ä¸ªå¹¶è¡Œçš„è‡ªæ³¨æ„åŠ›å±‚ï¼Œæ¯ä¸€ä¸ªè¿™ç§å±‚è¢«æˆä¸ºä¸€ä¸ªheadã€‚å¯¹æ¯ä¸ªå¤´æ¥è¯´ï¼Œåœ¨è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ä¹‹å‰ï¼Œæˆ‘ä»¬ä¼šå°†queryã€keyå’Œvalueç”¨ä¸‰ä¸ªç°è¡Œå±‚è¿›è¡Œæ˜ å°„ï¼Œè¿™$h$ä¸ªæ³¨æ„åŠ›å¤´çš„è¾“å‡ºå°†ä¼šè¢«æ‹¼æ¥ä¹‹åè¾“å…¥æœ€åä¸€ä¸ªçº¿æ€§å±‚è¿›è¡Œæ•´åˆã€‚\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5kpcsozid.png?imageView2/0/w/640/h/640)\n",
    "\n",
    "$$\n",
    "Fig.10.3.3\\ å¤šå¤´æ³¨æ„åŠ›\n",
    "$$\n",
    "\n",
    "\n",
    "å‡è®¾queryï¼Œkeyå’Œvalueçš„ç»´åº¦åˆ†åˆ«æ˜¯$d_q$ã€$d_k$å’Œ$d_v$ã€‚é‚£ä¹ˆå¯¹äºæ¯ä¸€ä¸ªå¤´$i=1,\\ldots,h$ï¼Œæˆ‘ä»¬å¯ä»¥è®­ç»ƒç›¸åº”çš„æ¨¡å‹æƒé‡$W_q^{(i)} \\in \\mathbb{R}^{p_q\\times d_q}$ã€$W_k^{(i)} \\in \\mathbb{R}^{p_k\\times d_k}$å’Œ$W_v^{(i)} \\in \\mathbb{R}^{p_v\\times d_v}$ï¼Œä»¥å¾—åˆ°æ¯ä¸ªå¤´çš„è¾“å‡ºï¼š\n",
    "\n",
    "\n",
    "$$\n",
    "o^{(i)} = attention(W_q^{(i)}q, W_k^{(i)}k, W_v^{(i)}v)\n",
    "$$\n",
    "\n",
    "\n",
    "è¿™é‡Œçš„attentionå¯ä»¥æ˜¯ä»»æ„çš„attention functionï¼Œæ¯”å¦‚å‰ä¸€èŠ‚ä»‹ç»çš„dot-product attentionä»¥åŠMLP attentionã€‚ä¹‹åæˆ‘ä»¬å°†æ‰€æœ‰headå¯¹åº”çš„è¾“å‡ºæ‹¼æ¥èµ·æ¥ï¼Œé€å…¥æœ€åä¸€ä¸ªçº¿æ€§å±‚è¿›è¡Œæ•´åˆï¼Œè¿™ä¸ªå±‚çš„æƒé‡å¯ä»¥è¡¨ç¤ºä¸º$W_o\\in \\mathbb{R}^{d_0 \\times hp_v}$\n",
    "\n",
    "\n",
    "$$\n",
    "o = W_o[o^{(1)}, \\ldots, o^{(h)}]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
