{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目标检测和边界框\n",
    "\n",
    "边界框"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bbox_to_rect(bbox, color):  # 本函数已保存在d2lzh_pytorch中方便以后使用\n",
    "    # 将边界框(左上x, 左上y, 右下x, 右下y)格式转换成matplotlib格式：\n",
    "    # ((左上x, 左上y), 宽, 高)\n",
    "    return d2l.plt.Rectangle(\n",
    "        xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],\n",
    "        fill=False, edgecolor=color, linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 锚框\n",
    "\n",
    "目标检测算法通常会在输入图像中采样大量的区域，然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边缘从而更准确地预测目标的真实边界框（ground-truth bounding box）。不同的模型使用的区域采样方法可能不同。这里我们介绍其中的一种方法：它以每个像素为中心生成多个大小和宽高比（aspect ratio）不同的边界框。这些边界框被称为锚框（anchor box）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们刚刚提到某个锚框较好地覆盖了图像中的狗。如果该目标的真实边界框已知，这里的“较好”该如何量化呢？一种直观的方法是衡量锚框和真实边界框之间的相似度。我们知道，Jaccard系数（Jaccard index）可以衡量两个集合的相似度。给定集合$\\mathcal{A}$和$\\mathcal{B}$，它们的Jaccard系数即二者交集大小除以二者并集大小：\n",
    "\n",
    "$$\n",
    "J(\\mathcal{A},\\mathcal{B}) = \\frac{\\left|\\mathcal{A} \\cap \\mathcal{B}\\right|}{\\left| \\mathcal{A} \\cup \\mathcal{B}\\right|}.\n",
    "$$\n",
    "\n",
    "\n",
    "实际上，我们可以把边界框内的像素区域看成是像素的集合。如此一来，我们可以用两个边界框的像素集合的Jaccard系数衡量这两个边界框的相似度。当衡量两个边界框的相似度时，我们通常将Jaccard系数称为交并比（Intersection over Union，IoU），即两个边界框相交面积与相并面积之比，如图9.2所示。交并比的取值范围在0和1之间：0表示两个边界框无重合像素，1表示两个边界框相等。\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5vs9jkw9f.png?imageView2/0/w/640/h/640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MultiBoxPrior(feature_map, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5]):\n",
    "    \"\"\"\n",
    "    # 按照「9.4.1. 生成多个锚框」所讲的实现, anchor表示成(xmin, ymin, xmax, ymax).\n",
    "    https://zh.d2l.ai/chapter_computer-vision/anchor.html\n",
    "    Args:\n",
    "        feature_map: torch tensor, Shape: [N, C, H, W].\n",
    "        sizes: List of sizes (0~1) of generated MultiBoxPriores. \n",
    "        ratios: List of aspect ratios (non-negative) of generated MultiBoxPriores. \n",
    "    Returns:\n",
    "        anchors of shape (1, num_anchors, 4). 由于batch里每个都一样, 所以第一维为1\n",
    "    \"\"\"\n",
    "    pairs = [] # pair of (size, sqrt(ration))\n",
    "    \n",
    "    # 生成n + m -1个框\n",
    "    for r in ratios:\n",
    "        pairs.append([sizes[0], math.sqrt(r)])\n",
    "    for s in sizes[1:]:\n",
    "        pairs.append([s, math.sqrt(ratios[0])])\n",
    "    \n",
    "    pairs = np.array(pairs)\n",
    "    \n",
    "    # 生成相对于坐标中心点的框（x,y,x,y）\n",
    "    ss1 = pairs[:, 0] * pairs[:, 1] # size * sqrt(ration)\n",
    "    ss2 = pairs[:, 0] / pairs[:, 1] # size / sqrt(ration)\n",
    "    \n",
    "    base_anchors = np.stack([-ss1, -ss2, ss1, ss2], axis=1) / 2\n",
    "    \n",
    "    #将坐标点和anchor组合起来生成hw（n+m-1）个框输出\n",
    "    h, w = feature_map.shape[-2:]\n",
    "    shifts_x = np.arange(0, w) / w\n",
    "    shifts_y = np.arange(0, h) / h\n",
    "    shift_x, shift_y = np.meshgrid(shifts_x, shifts_y)\n",
    "    \n",
    "    shift_x = shift_x.reshape(-1)\n",
    "    shift_y = shift_y.reshape(-1)\n",
    "    \n",
    "    shifts = np.stack((shift_x, shift_y, shift_x, shift_y), axis=1)\n",
    "    anchors = shifts.reshape((-1, 1, 4)) + base_anchors.reshape((1, -1, 4))\n",
    "    \n",
    "    return torch.tensor(anchors, dtype=torch.float32).view(1, -1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 交并比\n",
    "\n",
    "我们刚刚提到某个锚框较好地覆盖了图像中的狗。如果该目标的真实边界框已知，这里的“较好”该如何量化呢？一种直观的方法是衡量锚框和真实边界框之间的相似度。我们知道，Jaccard系数（Jaccard index）可以衡量两个集合的相似度。给定集合$\\mathcal{A}$和$\\mathcal{B}$，它们的Jaccard系数即二者交集大小除以二者并集大小：\n",
    "\n",
    "$$\n",
    "J(\\mathcal{A},\\mathcal{B}) = \\frac{\\left|\\mathcal{A} \\cap \\mathcal{B}\\right|}{\\left| \\mathcal{A} \\cup \\mathcal{B}\\right|}.\n",
    "$$\n",
    "\n",
    "\n",
    "实际上，我们可以把边界框内的像素区域看成是像素的集合。如此一来，我们可以用两个边界框的像素集合的Jaccard系数衡量这两个边界框的相似度。当衡量两个边界框的相似度时，我们通常将Jaccard系数称为交并比（Intersection over Union，IoU），即两个边界框相交面积与相并面积之比，如图9.2所示。交并比的取值范围在0和1之间：0表示两个边界框无重合像素，1表示两个边界框相等。\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5vs9jkw9f.png?imageView2/0/w/640/h/640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 以下函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "def compute_intersection(set_1, set_2):\n",
    "    \"\"\"\n",
    "    计算anchor之间的交集\n",
    "    Args:\n",
    "        set_1: a tensor of dimensions (n1, 4), anchor表示成(xmin, ymin, xmax, ymax)\n",
    "        set_2: a tensor of dimensions (n2, 4), anchor表示成(xmin, ymin, xmax, ymax)\n",
    "    Returns:\n",
    "        intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, shape: (n1, n2)\n",
    "    \"\"\"\n",
    "    # PyTorch auto-broadcasts singleton dimensions\n",
    "    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\n",
    "    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\n",
    "    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\n",
    "    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\n",
    "\n",
    "\n",
    "def compute_jaccard(set_1, set_2):\n",
    "    \"\"\"\n",
    "    计算anchor之间的Jaccard系数(IoU)\n",
    "    Args:\n",
    "        set_1: a tensor of dimensions (n1, 4), anchor表示成(xmin, ymin, xmax, ymax)\n",
    "        set_2: a tensor of dimensions (n2, 4), anchor表示成(xmin, ymin, xmax, ymax)\n",
    "    Returns:\n",
    "        Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, shape: (n1, n2)\n",
    "    \"\"\"\n",
    "    # Find intersections\n",
    "    intersection = compute_intersection(set_1, set_2)  # (n1, n2)\n",
    "\n",
    "    # Find areas of each box in both sets\n",
    "    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\n",
    "    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\n",
    "\n",
    "    # Find the union\n",
    "    # PyTorch auto-broadcasts singleton dimensions\n",
    "    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\n",
    "\n",
    "    return intersection / union  # (n1, n2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 样式迁移\n",
    "\n",
    "如果你是一位摄影爱好者，也许接触过滤镜。它能改变照片的颜色样式，从而使风景照更加锐利或者令人像更加美白。但一个滤镜通常只能改变照片的某个方面。如果要照片达到理想中的样式，经常需要尝试大量不同的组合，其复杂程度不亚于模型调参。\n",
    "\n",
    "在本节中，我们将介绍如何使用卷积神经网络自动将某图像中的样式应用在另一图像之上，即样式迁移（style transfer）[1]。这里我们需要两张输入图像，一张是内容图像，另一张是样式图像，我们将使用神经网络修改内容图像使其在样式上接近样式图像。图9.12中的内容图像为本书作者在西雅图郊区的雷尼尔山国家公园（Mount Rainier National Park）拍摄的风景照，而样式图像则是一副主题为秋天橡树的油画。最终输出的合成图像在保留了内容图像中物体主体形状的情况下应用了样式图像的油画笔触，同时也让整体颜色更加鲜艳。\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5w2i3mjvm.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "\n",
    "## 方法\n",
    "\n",
    "图9.13用一个例子来阐述基于卷积神经网络的样式迁移方法。首先，我们初始化合成图像，例如将其初始化成内容图像。该合成图像是样式迁移过程中唯一需要更新的变量，即样式迁移所需迭代的模型参数。然后，我们选择一个预训练的卷积神经网络来抽取图像的特征，其中的模型参数在训练中无须更新。深度卷积神经网络凭借多个层逐级抽取图像的特征。我们可以选择其中某些层的输出作为内容特征或样式特征。以图9.13为例，这里选取的预训练的神经网络含有3个卷积层，其中第二层输出图像的内容特征，而第一层和第三层的输出被作为图像的样式特征。接下来，我们通过正向传播（实线箭头方向）计算样式迁移的损失函数，并通过反向传播（虚线箭头方向）迭代模型参数，即不断更新合成图像。样式迁移常用的损失函数由3部分组成：内容损失（content loss）使合成图像与内容图像在内容特征上接近，样式损失（style loss）令合成图像与样式图像在样式特征上接近，而总变差损失（total variation loss）则有助于减少合成图像中的噪点。最后，当模型训练结束时，我们输出样式迁移的模型参数，即得到最终的合成图像。\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5w2jrvuc9.png?imageView2/0/w/640/h/640)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义损失函数\n",
    "\n",
    "下面我们来描述样式迁移的损失函数。它由内容损失、样式损失和总变差损失3部分组成。\n",
    "\n",
    "### 内容损失\n",
    "\n",
    "与线性回归中的损失函数类似，内容损失通过平方误差函数衡量合成图像与内容图像在内容特征上的差异。平方误差函数的两个输入均为`extract_features`函数计算所得到的内容层的输出。\n",
    "\n",
    "### 样式损失\n",
    "\n",
    "样式损失也一样通过平方误差函数衡量合成图像与样式图像在样式上的差异。为了表达样式层输出的样式，我们先通过`extract_features`函数计算样式层的输出。假设该输出的样本数为1，通道数为$c$，高和宽分别为$h$和$w$，我们可以把输出变换成$c$行$hw$列的矩阵$\\boldsymbol{X}$。矩阵$\\boldsymbol{X}$可以看作是由$c$个长度为$hw$的向量$\\boldsymbol{x}_1, \\ldots, \\boldsymbol{x}_c$组成的。其中向量$\\boldsymbol{x}_i$代表了通道$i$上的样式特征。这些向量的格拉姆矩阵（Gram matrix）$\\boldsymbol{X}\\boldsymbol{X}^\\top \\in \\mathbb{R}^{c \\times c}$中$i$行$j$列的元素$x_{ij}$即向量$\\boldsymbol{x}_i$与$\\boldsymbol{x}_j$的内积，它表达了通道$i$和通道$j$上样式特征的相关性。我们用这样的格拉姆矩阵表达样式层输出的样式。需要注意的是，当$hw$的值较大时，格拉姆矩阵中的元素容易出现较大的值。此外，格拉姆矩阵的高和宽皆为通道数$c$。为了让样式损失不受这些值的大小影响，下面定义的`gram`函数将格拉姆矩阵除以了矩阵中元素的个数，即$chw$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_loss(Y_hat, Y):\n",
    "    return F.mse_loss(Y_hat, Y)\n",
    "\n",
    "def gram(X):\n",
    "    num_channels, n = X.shape[1], X.shape[2] * X.shape[3]\n",
    "    X = X.view(num_channels, n)\n",
    "    return torch.matmul(X, X.t()) / (num_channels * n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总变差损失\n",
    "\n",
    "有时候，我们学到的合成图像里面有大量高频噪点，即有特别亮或者特别暗的颗粒像素。一种常用的降噪方法是总变差降噪（total variation denoising）。假设$x_{i,j}$表示坐标为$(i,j)$的像素值，降低总变差损失\n",
    "\n",
    "\n",
    "$$\n",
    "\\sum_{i,j} \\left|x_{i,j} - x_{i+1,j}\\right| + \\left|x_{i,j} - x_{i,j+1}\\right|\n",
    "$$\n",
    "\n",
    "\n",
    "能够尽可能使邻近的像素值相似。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 样式迁移常用的损失函数由3部分组成：内容损失使合成图像与内容图像在内容特征上接近，样式损失令合成图像与样式图像在样式特征上接近，而总变差损失则有助于减少合成图像中的噪点。\n",
    "* 可以通过预训练的卷积神经网络来抽取图像的特征，并通过最小化损失函数来不断更新合成图像。\n",
    "* 用格拉姆矩阵表达样式层输出的样式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
